{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379e6be2-3b9c-470f-bb8c-c4e9d5657ec6",
   "metadata": {},
   "source": [
    "### Directory Setup and Clearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b0593-c91b-46d0-9e9e-f78bb0ca4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ======================================\n",
    "# STEP 1: ONLY DEFINE PATHS (NO OPERATIONS)\n",
    "# ======================================\n",
    "\n",
    "# Base directory\n",
    "base_dir = \"/home/jovyan/__ANIMALS/datasets/ds8_auto_label_yolo_architectures\"\n",
    "\n",
    "# \n",
    "working_dirs = {\n",
    "    'train_images': os.path.join(base_dir, \"train/images\"),\n",
    "    'train_labels': os.path.join(base_dir, \"train/labels\"),\n",
    "    'val_images': os.path.join(base_dir, \"val/images\"), \n",
    "    'val_labels': os.path.join(base_dir, \"val/labels\"),\n",
    "    'augmented': os.path.join(base_dir, \"augmented\")  \n",
    "}\n",
    "\n",
    "# Original images (READ ONLY)\n",
    "original_images_dir = \"/home/jovyan/__ANIMALS/images\"\n",
    "\n",
    "# Verify paths (read-only check)\n",
    "print(\"Path verification:\")\n",
    "print(f\"Original images: {original_images_dir} (exists: {os.path.exists(original_images_dir)})\")\n",
    "print(f\"Base directory: {base_dir} (exists: {os.path.exists(base_dir)})\")\n",
    "\n",
    "# Show planned working directories\n",
    "print(\"\\nPlanned working directories:\")\n",
    "for name, path in working_dirs.items():\n",
    "    print(f\"{name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb99c89-8ee1-44b6-8e72-f6f437bfeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ======================================\n",
    "# SAFE DIRECTORY PREPARATION\n",
    "# ======================================\n",
    "\n",
    "def clean_and_create_dirs(dirs_to_clean):\n",
    "    \"\"\"Cleans and recreates ONLY specified directories\"\"\"\n",
    "    print(\"\\nDirectory preparation:\")\n",
    "    \n",
    "    # Only clean these specific directories\n",
    "    allowed_to_clean = {\n",
    "        'train_images', 'train_labels', \n",
    "        'val_images', 'val_labels',\n",
    "        'augmented'\n",
    "    }\n",
    "    \n",
    "    # Validate requested directories\n",
    "    invalid = set(dirs_to_clean) - allowed_to_clean\n",
    "    if invalid:\n",
    "        raise ValueError(f\"Cannot clean these directories: {invalid}\")\n",
    "    \n",
    "    # Process each directory\n",
    "    for dir_name in dirs_to_clean:\n",
    "        path = working_dirs[dir_name]\n",
    "        \n",
    "        # Clean if exists\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"✓ Cleared: {path}\")\n",
    "        \n",
    "        # Create fresh\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"✓ Created: {path}\")\n",
    "    \n",
    "    print(\"\\nOriginal images directory remains untouched:\")\n",
    "    print(f\"{original_images_dir} (exists: {os.path.exists(original_images_dir)})\")\n",
    "\n",
    "# List of directories to reset\n",
    "directories_to_reset = [\n",
    "    'train_images',\n",
    "    'train_labels', \n",
    "    'val_images',\n",
    "    'val_labels'\n",
    "    # Omit 'augmented' for now - we'll create it later if needed\n",
    "]\n",
    "\n",
    "# Execute cleaning\n",
    "try:\n",
    "    clean_and_create_dirs(directories_to_reset)\n",
    "    print(\"\\nSUCCESS: Working directories ready\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {str(e)}\")\n",
    "    print(\"Please verify the directory list and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784da85-0c73-488f-a447-0dd35af6184c",
   "metadata": {},
   "source": [
    "### Data Validation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cf468-ad19-41b0-9bb4-f42285749d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Paths\n",
    "json_file = \"result.json\"\n",
    "\n",
    "# Load JSON file\n",
    "with open(json_file, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract image and annotation data\n",
    "images = {img[\"id\"]: img[\"file_name\"].split(\"/\")[-1] for img in coco_data[\"images\"]}\n",
    "annotations = coco_data[\"annotations\"]\n",
    "categories = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n",
    "\n",
    "# Filter images to include only those with annotations\n",
    "annotated_images = set(ann[\"image_id\"] for ann in annotations)\n",
    "filtered_images = {img_id: images[img_id] for img_id in images if img_id in annotated_images}\n",
    "\n",
    "# Group images by category\n",
    "category_to_image_ids = {name: set() for name in categories.values()}\n",
    "\n",
    "for annotation in annotations:\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    category_id = annotation[\"category_id\"]\n",
    "    category_name = categories[category_id]\n",
    "    \n",
    "    # Add the image ID to the corresponding category\n",
    "    category_to_image_ids[category_name].add(image_id)\n",
    "\n",
    "# Convert sets to lists for splitting\n",
    "category_to_image_ids = {k: list(v) for k, v in category_to_image_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23af9cc-f048-4961-a2f8-bf8506191c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======================================\n",
    "# STEP 3: SAFE DATA VALIDATION\n",
    "# ======================================\n",
    "\n",
    "def validate_dataset(annotation_path):\n",
    "    \"\"\"Validates images without modifying originals\"\"\"\n",
    "    print(\"\\nStarting validation...\")\n",
    "    \n",
    "    # 1. Load annotations\n",
    "    try:\n",
    "        with open(annotation_path) as f:\n",
    "            coco_data = json.load(f)\n",
    "        print(f\"Loaded annotations with {len(coco_data['images'])} images\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading {annotation_path}: {str(e)}\")\n",
    "\n",
    "    # 2. Validate each image\n",
    "    valid_images = []\n",
    "    invalid_images = []\n",
    "    corrupt_files = []\n",
    "\n",
    "    for img in tqdm(coco_data['images'], desc=\"Validating\"):\n",
    "        try:\n",
    "            # Get actual filename (handles cases with prefixes)\n",
    "            filename = img['file_name'].split('__')[-1]\n",
    "            img_path = os.path.join(original_images_dir, filename)\n",
    "            \n",
    "            # Check existence\n",
    "            if not os.path.exists(img_path):\n",
    "                invalid_images.append(img['id'])\n",
    "                continue\n",
    "                \n",
    "            # Verify image integrity\n",
    "            with Image.open(img_path) as im:\n",
    "                im.verify()  # Verify without loading pixels\n",
    "                \n",
    "                # Check dimensions match metadata\n",
    "                if im.size != (img['width'], img['height']):\n",
    "                    print(f\"Size mismatch: {filename} (metadata {img['width']}x{img['height']} vs actual {im.size})\")\n",
    "                    invalid_images.append(img['id'])\n",
    "                    continue\n",
    "                    \n",
    "            valid_images.append(img)\n",
    "            \n",
    "        except Exception as e:\n",
    "            corrupt_files.append(filename)\n",
    "            invalid_images.append(img['id'])\n",
    "\n",
    "    # 3. Filter annotations\n",
    "    valid_annotations = [\n",
    "        ann for ann in coco_data['annotations'] \n",
    "        if ann['image_id'] not in invalid_images\n",
    "    ]\n",
    "    \n",
    "    # 4. Print report\n",
    "    print(\"\\nValidation Report:\")\n",
    "    print(f\"Valid images: {len(valid_images)}\")\n",
    "    print(f\"Invalid/corrupt: {len(invalid_images)}\")\n",
    "    if corrupt_files:\n",
    "        print(f\"Corrupt files (sample): {corrupt_files[:5]}\")\n",
    "\n",
    "    return coco_data, valid_images, valid_annotations\n",
    "\n",
    "# Run validation\n",
    "try:\n",
    "    coco_data, valid_images, valid_annotations = validate_dataset(\"result.json\")\n",
    "    print(\"\\nSUCCESS: Validation complete\")\n",
    "    print(f\"Ready to process {len(valid_images)} valid images\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {str(e)}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. result.json exists in current directory\")\n",
    "    print(\"2. Original images are accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca65e63-e305-4038-8f23-2a9d27c153b1",
   "metadata": {},
   "source": [
    "### Create Balanced Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c7a84-a30b-4ee5-a7c3-95e6bdd68e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_balanced_split(coco_data, valid_images, valid_annotations, output_dir):\n",
    "    \"\"\"Create balanced train/val split without modifying originals\"\"\"\n",
    "    # 1. Create output directories\n",
    "    os.makedirs(os.path.join(output_dir, \"train/images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"train/labels\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"val/images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"val/labels\"), exist_ok=True)\n",
    "\n",
    "    # Group images by category for balanced split\n",
    "    category_to_images = defaultdict(list)\n",
    "    for ann in valid_annotations:\n",
    "        category_to_images[ann['category_id']].append(ann['image_id'])\n",
    "\n",
    "    # Create balanced split (80% train, 20% val)\n",
    "    train_ids, val_ids = set(), set()\n",
    "    \n",
    "    for cat_id, img_ids in category_to_images.items():\n",
    "        # Ensure each class has at least 1 validation sample\n",
    "        if len(img_ids) > 4:  # Only split if we have enough samples\n",
    "            train, val = train_test_split(img_ids, test_size=0.2, random_state=42)\n",
    "            train_ids.update(train)\n",
    "            val_ids.update(val)\n",
    "        else:\n",
    "            # For rare classes, put all in training and we'll augment later\n",
    "            train_ids.update(img_ids)\n",
    "\n",
    "    # Copy files to new directories (not moving - preserving originals)\n",
    "    def copy_files(image_ids, split_name):\n",
    "        copied = 0\n",
    "        for img_id in tqdm(image_ids, desc=f\"Copying {split_name} images\"):\n",
    "            img_info = next(img for img in valid_images if img['id'] == img_id)\n",
    "            src_path = os.path.join(\"/home/jovyan/__ANIMALS/images\", \n",
    "                                  img_info['file_name'].split('__')[-1])\n",
    "            dest_path = os.path.join(output_dir, split_name, \"images\", \n",
    "                                   os.path.basename(src_path))\n",
    "            \n",
    "            # Copy image\n",
    "            shutil.copyfile(src_path, dest_path)\n",
    "            copied += 1\n",
    "        return copied\n",
    "\n",
    "    train_count = copy_files(train_ids, \"train\")\n",
    "    val_count = copy_files(val_ids, \"val\")\n",
    "\n",
    "    print(f\"\\nDataset split created at {output_dir}\")\n",
    "    print(f\"Training images: {train_count}\")\n",
    "    print(f\"Validation images: {val_count}\")\n",
    "    print(f\"Split ratio: {train_count/(train_count+val_count):.1%} train, {val_count/(train_count+val_count):.1%} val\")\n",
    "\n",
    "    return train_ids, val_ids\n",
    "\n",
    "# Usage:\n",
    "output_dir = \"/home/jovyan/__ANIMALS/datasets/ds8_auto_label_yolo_architectures/\"\n",
    "train_ids, val_ids = create_balanced_split(coco_data, valid_images, valid_annotations, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5ffff-e270-4965-b94a-d23679dceb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abf9c6d-8414-43bb-8544-6bb4db2dad85",
   "metadata": {},
   "source": [
    "### Create YOLO Format Labels and Verify Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f59ee-327a-4583-9d86-90c160824002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_yolo_labels(coco_data, valid_annotations, image_ids, output_dir, split_name):\n",
    "    \"\"\"Convert COCO annotations to YOLO format and analyze class distribution\"\"\"\n",
    "    # Prepare paths\n",
    "    label_dir = os.path.join(output_dir, split_name, \"labels\")\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    \n",
    "    # Create category mapping\n",
    "    cat_id_to_yolo = {cat['id']: idx for idx, cat in enumerate(sorted(coco_data['categories'], key=lambda x: x['id']))}\n",
    "    \n",
    "    # Convert annotations and track class counts\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for img_id in tqdm(image_ids, desc=f\"Creating {split_name} labels\"):\n",
    "        img_info = next(img for img in valid_images if img['id'] == img_id)\n",
    "        img_anns = [ann for ann in valid_annotations if ann['image_id'] == img_id]\n",
    "        \n",
    "        # Create label file\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_info['file_name'].split('__')[-1])[0] + '.txt')\n",
    "        \n",
    "        with open(label_path, 'w') as f:\n",
    "            for ann in img_anns:\n",
    "                # Convert bbox to YOLO format\n",
    "                x, y, w, h = ann['bbox']\n",
    "                img_w, img_h = img_info['width'], img_info['height']\n",
    "                x_center = (x + w/2) / img_w\n",
    "                y_center = (y + h/2) / img_h\n",
    "                width = w / img_w\n",
    "                height = h / img_h\n",
    "                \n",
    "                # Write to file\n",
    "                f.write(f\"{cat_id_to_yolo[ann['category_id']]} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "                \n",
    "                # Count classes\n",
    "                class_counts[ann['category_id']] += 1\n",
    "    \n",
    "    # Analyze and visualize class distribution\n",
    "    class_names = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    df = pd.DataFrame({\n",
    "        'Class': [class_names[cat_id] for cat_id in class_counts],\n",
    "        'Count': list(class_counts.values()),\n",
    "        'Split': split_name\n",
    "    }).sort_values('Count', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(df['Class'], df['Count'])\n",
    "    plt.title(f'Class Distribution - {split_name} Set')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Number of Instances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create labels for both sets\n",
    "train_df = create_yolo_labels(coco_data, valid_annotations, train_ids, output_dir, \"train\")\n",
    "val_df = create_yolo_labels(coco_data, valid_annotations, val_ids, output_dir, \"val\")\n",
    "\n",
    "# Show combined statistics\n",
    "combined_df = pd.concat([train_df, val_df])\n",
    "class_balance = combined_df.groupby('Class')['Count'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nClass Balance Report:\")\n",
    "print(class_balance)\n",
    "print(f\"\\nMost common class: {class_balance.idxmax()} ({class_balance.max()} instances)\")\n",
    "print(f\"Least common class: {class_balance.idxmin()} ({class_balance.min()} instances)\")\n",
    "print(f\"Imbalance ratio: {class_balance.max()/class_balance.min():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a1a76-649c-4566-a3d1-b421eef85670",
   "metadata": {},
   "source": [
    "### Targeted Augmentation for Rare Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fe130-fcaf-4058-bb05-3a4752656816",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc716922-2bf7-4f97-8d5b-991755391864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def augment_rare_classes(output_dir, class_counts, min_samples=50):\n",
    "    \"\"\"Augment rare classes until they reach minimum samples\"\"\"\n",
    "    # 1. Identify rare classes\n",
    "    rare_classes = [cat_id for cat_id, count in class_counts.items() if count < min_samples]\n",
    "    print(f\"Rare classes to augment: {[coco_data['categories'][cat_id]['name'] for cat_id in rare_classes]}\")\n",
    "    \n",
    "    # Define augmentations\n",
    "    transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "    ], bbox_params=A.BboxParams(format='yolo'))\n",
    "    \n",
    "    # Process each rare class\n",
    "    for cat_id in rare_classes:\n",
    "        class_name = coco_data['categories'][cat_id]['name']\n",
    "        print(f\"\\nAugmenting {class_name} (current: {class_counts[cat_id]} samples)\")\n",
    "        \n",
    "        # Find all images containing this class\n",
    "        train_label_dir = os.path.join(output_dir, \"train\", \"labels\")\n",
    "        image_files = [f for f in os.listdir(train_label_dir) if f.endswith('.txt')]\n",
    "        \n",
    "        samples_created = 0\n",
    "        for label_file in tqdm(image_files, desc=f\"Processing {class_name}\"):\n",
    "            with open(os.path.join(train_label_dir, label_file)) as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Check if this image contains the rare class\n",
    "            has_rare_class = any(int(line.split()[0]) == cat_id for line in lines)\n",
    "            if not has_rare_class:\n",
    "                continue\n",
    "            \n",
    "            # Load image and annotations\n",
    "            img_path = os.path.join(output_dir, \"train\", \"images\", label_file.replace('.txt', '.JPG'))\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Parse bounding boxes\n",
    "            bboxes = []\n",
    "            for line in lines:\n",
    "                class_id, xc, yc, w, h = map(float, line.split())\n",
    "                bboxes.append([xc, yc, w, h, int(class_id)])\n",
    "            \n",
    "            # Apply augmentations\n",
    "            augmented = transform(image=image, bboxes=bboxes)\n",
    "            \n",
    "            # Save new sample\n",
    "            new_img_path = os.path.join(output_dir, \"train\", \"images\", f\"aug_{class_name}_{samples_created}.JPG\")\n",
    "            cv2.imwrite(new_img_path, cv2.cvtColor(augmented['image'], cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Save new labels\n",
    "            with open(os.path.join(train_label_dir, f\"aug_{class_name}_{samples_created}.txt\"), 'w') as f:\n",
    "                for bbox in augmented['bboxes']:\n",
    "                    f.write(f\"{int(bbox[4])} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\")\n",
    "            \n",
    "            samples_created += 1\n",
    "            if class_counts[cat_id] + samples_created >= min_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Created {samples_created} new samples for {class_name}\")\n",
    "        class_counts[cat_id] += samples_created\n",
    "\n",
    "# Get current class counts (from previous step)\n",
    "class_counts = {cat['id']: 0 for cat in coco_data['categories']}\n",
    "for ann in valid_annotations:\n",
    "    class_counts[ann['category_id']] += 1\n",
    "\n",
    "# Run augmentation (target minimum 50 samples per class)\n",
    "augment_rare_classes(output_dir, class_counts, min_samples=50)\n",
    "\n",
    "# Verify new counts\n",
    "print(\"\\nUpdated class counts:\")\n",
    "for cat_id, count in sorted(class_counts.items(), key=lambda x: x[1]):\n",
    "    print(f\"{coco_data['categories'][cat_id]['name']}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e3b2d-23ef-4202-8a8b-2f3f38657656",
   "metadata": {},
   "source": [
    "### Optimized Training Configuration with Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e45038-ab02-4a09-9417-cef3e97ed462",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed228a56-b037-43bb-96fd-b7e34cb95421",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbacecf-871f-4eac-8ea6-4f190baeb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4af32-89ce-43f1-9a4e-00c81ba06522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. CPU Configuration\n",
    "# ======================\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\"  # Use 6 cores\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "torch.set_num_threads(6)  # Prevent overallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cc601-e68b-4dbc-b7ef-25ea7d6cb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. Class Weight Calculation\n",
    "# ======================\n",
    "class_counts = {\n",
    "    'badger': 173, 'boar': 172, 'brown_bear': 6, 'hare': 14, \n",
    "    'lynx': 93, 'musk_deer': 4, 'otter': 4, 'raccoon': 50,\n",
    "    'red_fox': 50, 'roe_deer_female': 314, 'roe_deer_male': 80,\n",
    "    'sable': 50, 'sika_deer_female': 1002, 'sika_deer_male': 214,\n",
    "    'tiger': 499, 'ussuri_bear': 8, 'wild_cat': 27, \n",
    "    'yellow_marten': 21\n",
    "}\n",
    "\n",
    "def calculate_cpu_weights(counts, power=0.75, max_weight=5.0):\n",
    "    median = np.median(list(counts.values()))\n",
    "    weights = {}\n",
    "    for cls, count in counts.items():\n",
    "        if count == 0:\n",
    "            weights[cls] = max_weight\n",
    "        else:\n",
    "            weight = (median / (count + 1)) ** power\n",
    "            weights[cls] = min(weight, max_weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    max_w = max(weights.values())\n",
    "    return {cls: w/max_w for cls, w in weights.items()}\n",
    "\n",
    "class_weights = calculate_cpu_weights(class_counts)\n",
    "print(\"CPU-Optimized Class Weights:\")\n",
    "for cls, weight in sorted(class_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{cls}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fdeb9-8a2a-497d-9448-2a0a4b690005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ======================\n",
    "# 2. Training Configuration (VALID PARAMS ONLY)\n",
    "# ======================\n",
    "output_dir = \"/home/jovyan/__ANIMALS/datasets/ds8_auto_label_yolo_architectures/\"\n",
    "\n",
    "training_config = {\n",
    "    \"data\": os.path.join(output_dir, \"dataset.yaml\"),\n",
    "    \"epochs\": 250,\n",
    "    \"batch\": 4,          # Optimized for CPU memory\n",
    "    \"imgsz\": 512,        # Reduced from 640\n",
    "    \"device\": \"cpu\",\n",
    "    \"workers\": 6,        # 6 cores dedicated\n",
    "    \"optimizer\": \"Adam\", # Better for CPU than AdamW\n",
    "    \"lr0\": 0.001,\n",
    "    \"lrf\": 0.01,\n",
    "    \"cls\": 3.0,          # Class loss weight\n",
    "    \"box\": 7.5,          # Box loss weight\n",
    "    \"augment\": True,     # Basic augmentations\n",
    "    \"fliplr\": 0.5,       # Horizontal flip\n",
    "    \"mosaic\": 0.3,       # Reduced for CPU\n",
    "    \"mixup\": 0.1,        # Reduced for CPU\n",
    "    \"close_mosaic\": 10,\n",
    "    \"resume\": True,\n",
    "    \"save_period\": 10,   # Save every 10 epochs\n",
    "    \"patience\": 75,      # Longer patience for CPU\n",
    "    \"name\": \"wildlife_cpu\",\n",
    "    \"overlap_mask\": True,\n",
    "    \"iou\": 0.6           # Slightly lower threshold\n",
    "}\n",
    "\n",
    "# Remove unsupported parameters\n",
    "training_config.pop('fl_gamma', None)\n",
    "training_config.pop('copy_paste', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b278b-a984-4d3b-8bdd-1ce6c5c3647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Training Phases Manager\n",
    "# ======================\n",
    "class PhaseManager:\n",
    "    def __init__(self):\n",
    "        self.phases = {\n",
    "            0: {\"mosaic\": 0.0, \"mixup\": 0.0, \"lr\": 0.001},    # Epochs 1-75\n",
    "            75: {\"mosaic\": 0.3, \"mixup\": 0.1, \"lr\": 0.0002},   # Epochs 76-150\n",
    "            150: {\"mosaic\": 0.1, \"mixup\": 0.05, \"lr\": 0.00005} # Epochs 151-250\n",
    "        }\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer):\n",
    "        current_epoch = trainer.epoch\n",
    "        for start_epoch, config in sorted(self.phases.items(), reverse=True):\n",
    "            if current_epoch >= start_epoch:\n",
    "                trainer.args.mosaic = config[\"mosaic\"]\n",
    "                trainer.args.mixup = config[\"mixup\"]\n",
    "                trainer.lr = config[\"lr\"]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353d578-8d81-42de-a426-2eaacd323ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5. CPU Monitoring\n",
    "# ======================\n",
    "class CPUMonitor:\n",
    "    def __init__(self):\n",
    "        self.best_ap = 0\n",
    "        self.class_history = defaultdict(list)\n",
    "    \n",
    "    def on_val_end(self, trainer):\n",
    "        # Track best AP\n",
    "        current_ap = trainer.metrics.ap50\n",
    "        if current_ap > self.best_ap:\n",
    "            self.best_ap = current_ap\n",
    "        \n",
    "        # Track class-wise performance\n",
    "        for i, ap in enumerate(trainer.metrics.ap_class):\n",
    "            cls_name = trainer.data['names'][i]\n",
    "            self.class_history[cls_name].append(ap)\n",
    "        \n",
    "        # Print diagnostics\n",
    "        print(f\"\\n[Epoch {trainer.epoch}] Best mAP50: {self.best_ap:.3f}\")\n",
    "        print(\"Top 5 improving classes:\")\n",
    "        improving = sorted([(k, v[-1]-v[-2]) for k,v in self.class_history.items() \n",
    "                          if len(v) > 1], key=lambda x: x[1], reverse=True)[:5]\n",
    "        for cls, gain in improving:\n",
    "            print(f\"{cls}: +{gain:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107fb34-60f9-47c0-9318-130b8e5e44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. Training Execution\n",
    "# ======================\n",
    "def train_model():\n",
    "    # Initialize\n",
    "    model = YOLO(\"yolov8s.pt\")\n",
    "    \n",
    "    # Add callbacks\n",
    "    phase_manager = PhaseManager()\n",
    "    model.add_callback(\"on_train_epoch_start\", phase_manager.on_train_epoch_start)\n",
    "    \n",
    "    # Check for existing checkpoints\n",
    "    checkpoint_dir = os.path.join(\"runs\", \"detect\", training_config[\"name\"])\n",
    "    last_checkpoint = None\n",
    "    \n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) \n",
    "                      if f.endswith(\".pt\") and \"temp\" not in f]\n",
    "        if checkpoints:\n",
    "            checkpoints.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)))\n",
    "            last_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "            print(f\"Resuming from: {last_checkpoint}\")\n",
    "            training_config[\"resume\"] = last_checkpoint\n",
    "    \n",
    "    # Start training with error handling\n",
    "    try:\n",
    "        results = model.train(**training_config)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        print(\"Attempting minimal configuration...\")\n",
    "        minimal_config = {\n",
    "            \"data\": training_config[\"data\"],\n",
    "            \"epochs\": training_config[\"epochs\"],\n",
    "            \"batch\": 2,\n",
    "            \"imgsz\": 512,\n",
    "            \"device\": \"cpu\",\n",
    "            \"workers\": 4,\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"lr0\": 0.001,\n",
    "            \"name\": training_config[\"name\"] + \"_minimal\"\n",
    "        }\n",
    "        results = model.train(**minimal_config)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bb2fc-5dc4-43d5-b7ba-0f9272733a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch > 200 and trainer.metrics.ap50 > 0.8:\n",
    "    trainer.args.val_augment = True  # TTA\n",
    "    trainer.args.conf = 0.001        # Lower confidence threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
