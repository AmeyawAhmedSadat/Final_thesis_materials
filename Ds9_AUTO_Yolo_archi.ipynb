{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314d3de-25d0-44ff-8e08-3848b258bbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "379e6be2-3b9c-470f-bb8c-c4e9d5657ec6",
   "metadata": {},
   "source": [
    "### Directory Setup, Data Loading, and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c71f76-ba2f-47ae-884f-d24884611093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325a0d3-59b7-4f51-9fd0-36bc695ab42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b018b-a853-4479-b276-9c6bafb15a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Setup directories\n",
    "def setup_directories(base_dir):\n",
    "    dirs = {\n",
    "        'train_images': os.path.join(base_dir, \"train/images\"),\n",
    "        'train_labels': os.path.join(base_dir, \"train/labels\"),\n",
    "        'val_images': os.path.join(base_dir, \"val/images\"),\n",
    "        'val_labels': os.path.join(base_dir, \"val/labels\"),\n",
    "    }\n",
    "    for dir_path in dirs.values():\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Removed existing directory: {dir_path}\")\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    return dirs\n",
    "\n",
    "#  Load and validate COCO data\n",
    "def load_and_validate_coco(json_path, images_dir):\n",
    "    print(\"Loading COCO annotations...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Map image IDs to filenames\n",
    "    image_id_to_file = {img['id']: img['file_name'].split('__')[-1] for img in coco_data['images']}\n",
    "    \n",
    "    # Validate images\n",
    "    valid_images = []\n",
    "    invalid_images = []\n",
    "    print(\"Validating images...\")\n",
    "    for img_id, filename in tqdm(image_id_to_file.items(), desc=\"Validating Images\"):\n",
    "        img_path = os.path.join(images_dir, filename)\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                invalid_images.append(img_id)\n",
    "                continue\n",
    "            with Image.open(img_path) as im:\n",
    "                im.verify()  # Check if image is corrupt\n",
    "            valid_images.append(img_id)\n",
    "        except Exception as e:\n",
    "            invalid_images.append(img_id)\n",
    "            print(f\"Invalid image {filename}: {str(e)}\")\n",
    "    \n",
    "    # Filter annotations for valid images\n",
    "    valid_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in valid_images]\n",
    "    \n",
    "    # Initial EDA: Class distribution\n",
    "    class_counts = defaultdict(int)\n",
    "    for ann in valid_annotations:\n",
    "        class_counts[ann['category_id']] += 1\n",
    "    \n",
    "    print(\"\\nClass distribution in full dataset:\")\n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    for cat_id, count in sorted(class_counts.items()):\n",
    "        print(f\"{category_id_to_name[cat_id]} (ID {cat_id}): {count} annotations\")\n",
    "    \n",
    "    print(f\"\\nTotal images: {len(coco_data['images'])}\")\n",
    "    print(f\"Valid images: {len(valid_images)}\")\n",
    "    print(f\"Invalid images: {len(invalid_images)}\")\n",
    "    print(f\"Total annotations: {len(coco_data['annotations'])}\")\n",
    "    print(f\"Valid annotations: {len(valid_annotations)}\")\n",
    "    \n",
    "    return coco_data, image_id_to_file, valid_images, valid_annotations\n",
    "\n",
    "# Usage\n",
    "base_dir = \"/home/jovyan/__ANIMALS/datasets/Y9_AUTO_FIN/\"\n",
    "images_dir = \"/home/jovyan/__ANIMALS/images\"\n",
    "json_path = \"result.json\" \n",
    "\n",
    "dataset_dirs = setup_directories(base_dir)\n",
    "coco_data, image_id_to_file, valid_images, valid_annotations = load_and_validate_coco(json_path, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9aa6a-3513-46f9-a211-6477e9045c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#  Load and validate COCO data\n",
    "def load_and_validate_coco(json_path, images_dir):\n",
    "    print(\"Loading COCO annotations...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Map image IDs to filenames\n",
    "    image_id_to_file = {img['id']: img['file_name'].split('__')[-1] for img in coco_data['images']}\n",
    "    \n",
    "    # Validate images\n",
    "    valid_images = []\n",
    "    invalid_images = []\n",
    "    print(\"Validating images...\")\n",
    "    for img_id, filename in tqdm(image_id_to_file.items(), desc=\"Validating Images\"):\n",
    "        img_path = os.path.join(images_dir, filename)\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                invalid_images.append(img_id)\n",
    "                continue\n",
    "            with Image.open(img_path) as im:\n",
    "                im.verify()  # Check if image is corrupt\n",
    "            valid_images.append(img_id)\n",
    "        except Exception as e:\n",
    "            invalid_images.append(img_id)\n",
    "            print(f\"Invalid image {filename}: {str(e)}\")\n",
    "    \n",
    "    # Filter annotations for valid images\n",
    "    valid_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in valid_images]\n",
    "    \n",
    "    # Initial EDA: Class distribution\n",
    "    class_counts = defaultdict(int)\n",
    "    for ann in valid_annotations:\n",
    "        class_counts[ann['category_id']] += 1\n",
    "    \n",
    "    print(\"\\nClass distribution in full dataset:\")\n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    for cat_id, count in sorted(class_counts.items()):\n",
    "        print(f\"{category_id_to_name[cat_id]} (ID {cat_id}): {count} annotations\")\n",
    "    \n",
    "    print(f\"\\nTotal images: {len(coco_data['images'])}\")\n",
    "    print(f\"Valid images: {len(valid_images)}\")\n",
    "    print(f\"Invalid images: {len(invalid_images)}\")\n",
    "    print(f\"Total annotations: {len(coco_data['annotations'])}\")\n",
    "    print(f\"Valid annotations: {len(valid_annotations)}\")\n",
    "    \n",
    "    return coco_data, image_id_to_file, valid_images, valid_annotations\n",
    "\n",
    "# Usage\n",
    "base_dir = \"/home/jovyan/__ANIMALS/datasets/Y9_AUTO_FIN/\"\n",
    "images_dir = \"/home/jovyan/__ANIMALS/images\"\n",
    "json_path = \"result.json\" \n",
    "\n",
    "\n",
    "coco_data, image_id_to_file, valid_images, valid_annotations = load_and_validate_coco(json_path, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d8df2-2616-4841-a148-878ee63ae35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_coco_class_distribution(json_path):\n",
    "    \"\"\"Analyze class distribution in COCO format dataset\"\"\"\n",
    "    \n",
    "    print(\"Loading COCO annotations...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create mappings\n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    \n",
    "    # Calculate class statistics\n",
    "    class_stats = defaultdict(lambda: {'images': set(), 'instances': 0})\n",
    "    \n",
    "    print(\"\\nCounting instances...\")\n",
    "    for ann in tqdm(coco_data['annotations'], desc=\"Processing annotations\"):\n",
    "        class_stats[ann['category_id']]['images'].add(ann['image_id'])\n",
    "        class_stats[ann['category_id']]['instances'] += 1\n",
    "    \n",
    "    total_instances = sum(stats['instances'] for stats in class_stats.values())\n",
    "    \n",
    "    # Print class distribution table\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(\"{:<20} {:<10} {:<10} {:<10}\".format(\"Class\", \"Images\", \"Instances\", \"% of Total\"))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Sort by instance count (descending)\n",
    "    sorted_classes = sorted(\n",
    "        class_stats.items(),\n",
    "        key=lambda x: x[1]['instances'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for cat_id, stats in sorted_classes:\n",
    "        cat_name = category_id_to_name[cat_id]\n",
    "        percent = (stats['instances'] / total_instances) * 100 if total_instances > 0 else 0\n",
    "        print(\"{:<20} {:<10} {:<10} {:<10.1f}%\".format(\n",
    "            cat_name,\n",
    "            len(stats['images']),\n",
    "            stats['instances'],\n",
    "            percent\n",
    "        ))\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total classes: {len(coco_data['categories'])}\")\n",
    "    print(f\"Total images: {len(coco_data['images'])}\")\n",
    "    print(f\"Total annotations: {len(coco_data['annotations'])}\")\n",
    "    print(f\"Total instances: {total_instances}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = \"result.json\" \n",
    "    analyze_coco_class_distribution(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9164fb-52b1-4a4d-b74b-2504c5a5e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_class_distribution(coco_data):\n",
    "    # Create mapping from category ID to name\n",
    "    id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    \n",
    "    # Count annotations per class\n",
    "    class_counts = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        class_name = id_to_name[ann['category_id']]\n",
    "        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "    \n",
    "    # Count unique images per class\n",
    "    images_per_class = {}\n",
    "    for class_id in id_to_name:\n",
    "        class_name = id_to_name[class_id]\n",
    "        # Get all image IDs containing this class\n",
    "        class_images = set(ann['image_id'] for ann in coco_data['annotations'] \n",
    "                          if ann['category_id'] == class_id)\n",
    "        images_per_class[class_name] = len(class_images)\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    df = pd.DataFrame({\n",
    "        'Class': list(images_per_class.keys()),\n",
    "        'Image Count': list(images_per_class.values()),\n",
    "        'Annotation Count': [class_counts.get(cls, 0) for cls in images_per_class.keys()]\n",
    "    }).sort_values('Image Count', ascending=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nTotal Images in Dataset: {len(coco_data['images'])}\")\n",
    "    print(f\"Total Annotations: {len(coco_data['annotations'])}\")\n",
    "    print(f\"Unique Classes: {len(df)}\")\n",
    "    \n",
    "    print(\"\\nImages per Class (Top 10):\")\n",
    "    print(df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nClasses with Fewest Images:\")\n",
    "    print(df.tail(10).to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=df, y='Class', x='Image Count', palette='viridis')\n",
    "    plt.title('Number of Images per Class', fontsize=16)\n",
    "    plt.xlabel('Number of Images', fontsize=12)\n",
    "    plt.ylabel('Animal Class', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the analysis\n",
    "class_distribution = analyze_class_distribution(coco_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e27db-2eaf-4d46-8b4e-52f37ea26e06",
   "metadata": {},
   "source": [
    "## Observations from EDA\n",
    "- Total Images: 1960\n",
    "- Total Annotations: 2192 (some images have multiple annotations)\n",
    "- Classes Present: 18 out of 21 (missing \"red_squirrel\", \"wolf\", \"wolverine\")\n",
    "### Imbalance:\n",
    "- High-frequency classes: \"tiger\" (724), \"roe_deer_female\" (386), \"sika_deer_female\" (224)\n",
    "- Low-frequency classes: \"otter\" (1), \"brown_bear\" (4), \"hare\" (7), \"ussuri_bear\" (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d48f0-fcae-4241-8fdb-bb0390fbd81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969669be-fa7f-43c8-a416-5607a6875a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "095ec3d3-3b6c-48be-97f8-90d5ddaa8aaf",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def1c3-f32c-44df-8086-f2217bfdf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def split_coco_dataset(coco_data, valid_images, valid_annotations, train_size=0.8):\n",
    "    images = [img for img in coco_data['images'] if img['id'] in valid_images]\n",
    "    annotations = valid_annotations\n",
    "    categories = coco_data['categories']\n",
    "    info = coco_data['info']\n",
    "    \n",
    "    # Map image IDs to their annotations\n",
    "    image_to_anns = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        image_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "    # Group images by category (based on annotations)\n",
    "    category_to_images = defaultdict(set)\n",
    "    for img in images:\n",
    "        img_id = img['id']\n",
    "        for ann in image_to_anns[img_id]:\n",
    "            category_to_images[ann['category_id']].add(img_id)\n",
    "    \n",
    "    # Split images per category with rare class handling\n",
    "    train_image_ids = set()\n",
    "    val_image_ids = set()\n",
    "    for category_id, img_ids in category_to_images.items():\n",
    "        img_list = list(img_ids)\n",
    "        random.shuffle(img_list)\n",
    "        num_images = len(img_list)\n",
    "        \n",
    "        if num_images == 1:\n",
    "            train_image_ids.update(img_list)\n",
    "            print(f\"Class {category_id} has 1 image, assigned to train.\")\n",
    "        elif num_images < 5:\n",
    "            split_index = max(1, num_images - 1)  # At least 1 in val\n",
    "            train_image_ids.update(img_list[:split_index])\n",
    "            val_image_ids.update(img_list[split_index:])\n",
    "            print(f\"Class {category_id} has {num_images} images: {split_index} train, {num_images - split_index} val.\")\n",
    "        else:\n",
    "            split_index = int(num_images * train_size)\n",
    "            train_image_ids.update(img_list[:split_index])\n",
    "            val_image_ids.update(img_list[split_index:])\n",
    "    \n",
    "    # Resolve overlaps: prioritize train for rare classes, move conflicts to val\n",
    "    overlap = train_image_ids & val_image_ids\n",
    "    if overlap:\n",
    "        print(f\"Found {len(overlap)} overlapping images. Resolving...\")\n",
    "        for img_id in overlap:\n",
    "            anns = image_to_anns[img_id]\n",
    "            min_ann_count = min(len(image_to_anns[img_id]) for img_id in image_to_anns)\n",
    "            if any(ann['category_id'] in [cat['id'] for cat in categories if len(category_to_images[cat['id']]) < 5] for ann in anns):\n",
    "                val_image_ids.remove(img_id)  # Keep in train for rare classes\n",
    "            else:\n",
    "                train_image_ids.remove(img_id)  # Move to val otherwise\n",
    "    \n",
    "    # Finalize train and val sets\n",
    "    train_images = [img for img in images if img['id'] in train_image_ids]\n",
    "    val_images = [img for img in images if img['id'] in val_image_ids]\n",
    "    train_anns = [ann for ann in annotations if ann['image_id'] in train_image_ids]\n",
    "    val_anns = [ann for ann in annotations if ann['image_id'] in val_image_ids]\n",
    "    \n",
    "    train_data = {\n",
    "        'info': info,\n",
    "        'images': train_images,\n",
    "        'annotations': train_anns,\n",
    "        'categories': categories\n",
    "    }\n",
    "    val_data = {\n",
    "        'info': info,\n",
    "        'images': val_images,\n",
    "        'annotations': val_anns,\n",
    "        'categories': categories\n",
    "    }\n",
    "    \n",
    "    # EDA: Class distribution\n",
    "    def print_class_distribution(annotations, split_name, category_id_to_name):\n",
    "        class_counts = defaultdict(int)\n",
    "        for ann in annotations:\n",
    "            class_counts[ann['category_id']] += 1\n",
    "        print(f\"\\nClass distribution in {split_name} set:\")\n",
    "        for cat_id in sorted(class_counts.keys()):\n",
    "            print(f\"{category_id_to_name[cat_id]} (ID {cat_id}): {class_counts[cat_id]} annotations\")\n",
    "        print(f\"Total {split_name} images: {len(train_data['images']) if split_name == 'train' else len(val_data['images'])}\")\n",
    "        print(f\"Total {split_name} annotations: {len(annotations)}\")\n",
    "    \n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "    print_class_distribution(train_anns, \"train\", category_id_to_name)\n",
    "    print_class_distribution(val_anns, \"val\", category_id_to_name)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Usage\n",
    "train_data, val_data = split_coco_dataset(coco_data, valid_images, valid_annotations, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed79dd-f5a9-4d7a-8dda-4c64a0039c98",
   "metadata": {},
   "source": [
    "### Copy Images and Generate YOLO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028c1d9-cd0f-4759-ba28-dee0fe9bb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to copy images\n",
    "def copy_images(images_dir, dataset_dirs, train_image_ids, val_image_ids, image_id_to_file):\n",
    "    def copy_subset(image_ids, subset_name):\n",
    "        for img_id in tqdm(image_ids, desc=f\"Copying {subset_name} Images\"):\n",
    "            if img_id not in image_id_to_file:\n",
    "                print(f\"Warning: Image ID {img_id} not found in image_id_to_file.\")\n",
    "                continue\n",
    "            filename = image_id_to_file[img_id]\n",
    "            src = os.path.join(images_dir, filename)\n",
    "            dst = os.path.join(dataset_dirs[f\"{subset_name}_images\"], filename)\n",
    "            if os.path.exists(src):\n",
    "                shutil.copyfile(src, dst)\n",
    "            else:\n",
    "                print(f\"Warning: {subset_name} image {filename} not found at {src}.\")\n",
    "    \n",
    "    copy_subset(train_image_ids, \"train\")\n",
    "    copy_subset(val_image_ids, \"val\")\n",
    "\n",
    "# Function to convert COCO to YOLO format\n",
    "def prepare_yolo_labels(dataset_dirs, train_data, val_data, image_id_to_file):\n",
    "    def convert_to_yolo(image_ids, annotations, split_name):\n",
    "        for img_id in tqdm(image_ids, desc=f\"Preparing {split_name} labels\"):\n",
    "            filename = image_id_to_file[img_id]\n",
    "            img_info = next(img for img in coco_data['images'] if img['id'] == img_id)\n",
    "            label_path = os.path.join(dataset_dirs[f\"{split_name}_labels\"], f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            \n",
    "            with open(label_path, 'w') as f:\n",
    "                img_anns = [ann for ann in annotations if ann['image_id'] == img_id]\n",
    "                for ann in img_anns:\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    img_w, img_h = img_info['width'], img_info['height']\n",
    "                    x_center = (x + w / 2) / img_w\n",
    "                    y_center = (y + h / 2) / img_h\n",
    "                    width = w / img_w\n",
    "                    height = h / img_h\n",
    "                    f.write(f\"{ann['category_id']} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "    \n",
    "    train_image_ids = [img['id'] for img in train_data['images']]\n",
    "    val_image_ids = [img['id'] for img in val_data['images']]\n",
    "    convert_to_yolo(train_image_ids, train_data['annotations'], \"train\")\n",
    "    convert_to_yolo(val_image_ids, val_data['annotations'], \"val\")\n",
    "\n",
    "# Usage\n",
    "images_dir = \"/home/jovyan/__ANIMALS/images\"\n",
    "train_image_ids = [img['id'] for img in train_data['images']]\n",
    "val_image_ids = [img['id'] for img in val_data['images']]\n",
    "\n",
    "copy_images(images_dir, dataset_dirs, train_image_ids, val_image_ids, image_id_to_file)\n",
    "prepare_yolo_labels(dataset_dirs, train_data, val_data, image_id_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c8258-6d42-45c8-9467-3776b4ef6842",
   "metadata": {},
   "source": [
    "##  Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a4144-714c-4dcf-abcf-86c67eecc9ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def oversample_rare_classes(dataset_dirs, train_data, target_count=10):\n",
    "    # Map image IDs to annotations\n",
    "    image_to_anns = defaultdict(list)\n",
    "    for ann in train_data['annotations']:\n",
    "        image_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "    # Count annotations per class in train\n",
    "    class_counts = defaultdict(int)\n",
    "    for ann in train_data['annotations']:\n",
    "        class_counts[ann['category_id']] += 1\n",
    "    \n",
    "    # Identify images to oversample\n",
    "    rare_classes = {cat_id: count for cat_id, count in class_counts.items() if count < target_count}\n",
    "    print(\"Rare classes to oversample:\", {cat_id: count for cat_id, count in rare_classes.items()})\n",
    "    \n",
    "    # Oversample\n",
    "    train_images_dir = dataset_dirs['train_images']\n",
    "    train_labels_dir = dataset_dirs['train_labels']\n",
    "    \n",
    "    for cat_id, current_count in rare_classes.items():\n",
    "        needed = target_count - current_count\n",
    "        if needed <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Get images with this category\n",
    "        candidate_images = [img for img in train_data['images'] if any(ann['category_id'] == cat_id for ann in image_to_anns[img['id']])]\n",
    "        print(f\"Oversampling class {cat_id}: {current_count} → {target_count}, adding {needed} instances.\")\n",
    "        \n",
    "        for i in range(needed):\n",
    "            img = candidate_images[i % len(candidate_images)]  # Cycle through available images\n",
    "            filename = os.path.basename(img['file_name'].split('__')[-1])\n",
    "            src_img = os.path.join(train_images_dir, filename)\n",
    "            src_label = os.path.join(train_labels_dir, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            dst_img = os.path.join(train_images_dir, f\"{os.path.splitext(filename)[0]}_oversample_{i}.jpg\")\n",
    "            dst_label = os.path.join(train_labels_dir, f\"{os.path.splitext(filename)[0]}_oversample_{i}.txt\")\n",
    "            \n",
    "            shutil.copyfile(src_img, dst_img)\n",
    "            shutil.copyfile(src_label, dst_label)\n",
    "    \n",
    "    # Verify new counts\n",
    "    new_files = [f for f in os.listdir(train_labels_dir) if f.endswith('.txt')]\n",
    "    new_counts = defaultdict(int)\n",
    "    for f in tqdm(new_files, desc=\"Verifying oversampled train set\"):\n",
    "        with open(os.path.join(train_labels_dir, f), 'r') as label_file:\n",
    "            for line in label_file:\n",
    "                cat_id = int(line.split()[0])\n",
    "                new_counts[cat_id] += 1\n",
    "    \n",
    "    print(\"\\nNew class distribution in train set after oversampling:\")\n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in train_data['categories']}\n",
    "    for cat_id in sorted(new_counts.keys()):\n",
    "        print(f\"{category_id_to_name[cat_id]} (ID {cat_id}): {new_counts[cat_id]} annotations\")\n",
    "    print(f\"Total train images after oversampling: {len(os.listdir(train_images_dir))}\")\n",
    "\n",
    "# Usage\n",
    "oversample_rare_classes(dataset_dirs, train_data, target_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1337393-9f30-402e-9f95-a01b48cf981e",
   "metadata": {},
   "source": [
    "## Regenerate YOLO Labels with Remapped IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2dd5e-cf47-44ca-82a8-48b2f7ef551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_yolo_labels(dataset_dirs, train_data, val_data, image_id_to_file):\n",
    "    # Define COCO to YOLO ID mapping\n",
    "    coco_to_yolo = {\n",
    "        0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8,\n",
    "        10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 20: 17\n",
    "    }\n",
    "    \n",
    "    def convert_to_yolo(image_ids, annotations, split_name):\n",
    "        for img_id in tqdm(image_ids, desc=f\"Preparing {split_name} labels\"):\n",
    "            filename = image_id_to_file[img_id]\n",
    "            img_info = next(img for img in coco_data['images'] if img['id'] == img_id)\n",
    "            label_path = os.path.join(dataset_dirs[f\"{split_name}_labels\"], f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            \n",
    "            with open(label_path, 'w') as f:\n",
    "                img_anns = [ann for ann in annotations if ann['image_id'] == img_id]\n",
    "                for ann in img_anns:\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    img_w, img_h = img_info['width'], img_info['height']\n",
    "                    x_center = (x + w / 2) / img_w\n",
    "                    y_center = (y + h / 2) / img_h\n",
    "                    width = w / img_w\n",
    "                    height = h / img_h\n",
    "                    yolo_id = coco_to_yolo[ann['category_id']]\n",
    "                    f.write(f\"{yolo_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "    \n",
    "    # Regenerate train labels (including oversampled files)\n",
    "    train_labels_dir = dataset_dirs['train_labels']\n",
    "    for label_file in os.listdir(train_labels_dir):\n",
    "        os.remove(os.path.join(train_labels_dir, label_file))  # Clear existing labels\n",
    "    \n",
    "    train_image_ids = [img['id'] for img in train_data['images']]\n",
    "    convert_to_yolo(train_image_ids, train_data['annotations'], \"train\")\n",
    "    \n",
    "    # Regenerate val labels\n",
    "    val_labels_dir = dataset_dirs['val_labels']\n",
    "    for label_file in os.listdir(val_labels_dir):\n",
    "        os.remove(os.path.join(val_labels_dir, label_file))  # Clear existing labels\n",
    "    \n",
    "    val_image_ids = [img['id'] for img in val_data['images']]\n",
    "    convert_to_yolo(val_image_ids, val_data['annotations'], \"val\")\n",
    "\n",
    "# Usage\n",
    "prepare_yolo_labels(dataset_dirs, train_data, val_data, image_id_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f234e-6aa3-4ae2-a5d9-7c85ca35fec9",
   "metadata": {},
   "source": [
    "### YOLO MODEL \n",
    "\n",
    "#### YOLO 8S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd526f-def9-4f81-a661-8ce204f408d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model_name = \"yolov8s.pt\"\n",
    "if not os.path.exists(model_name):\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "else:\n",
    "    print(f\"Loading {model_name}...\")\n",
    "\n",
    "model = YOLO(model_name)\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data=\"/home/jovyan/__ANIMALS/datasets/Y9_AUTO_FIN/animals.yaml\",\n",
    "    epochs=100,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    device=\"cpu\",  # Change to \"0\" if GPU is available\n",
    "    patience=10,\n",
    "    save_period=5,\n",
    "    pretrained=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr0=0.001,\n",
    "    cos_lr=True,\n",
    "    augment=True,\n",
    "    mosaic=1.0,      # Combine four images into one\n",
    "    hsv_h=0.015,     # Hue adjustment\n",
    "    hsv_s=0.7,       # Saturation adjustment\n",
    "    hsv_v=0.4,       # Value (brightness) adjustment\n",
    "    flipud=0.5,      # 50% chance of flipping upside down\n",
    "    fliplr=0.5,      # 50% chance of flipping left-right (corrected from flip_lr)\n",
    "    degrees=10.0,    # Random rotation up to ±10 degrees (corrected from rotate)\n",
    "    translate=0.1,   # Translate by 10% of image size\n",
    "    scale=0.5,       # Scale by up to 50%\n",
    "    cls=0.5,         # Weight for classification loss (to help with imbalance)\n",
    "    box=7.5,         # Weight for bounding box loss\n",
    "    dfl=1.5          # Weight for distribution focal loss\n",
    ")\n",
    "\n",
    "print(\"Training completed. Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9114c-6f01-4efb-8fd9-c4fe427eed09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d10b1-c9da-45a4-821f-36e4c5fe3641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a2420-3240-40fa-ba12-674bc051d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Run validation\n",
    "results = model.val(\n",
    "        data='animals.yaml',\n",
    "        split='val',\n",
    "        save_json=True,\n",
    "        save_conf=True,\n",
    "        save_txt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc4887-032d-4230-90a6-18c80f51eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the training run\n",
    "model = YOLO(\"runs/detect/YOLO_5su_fin//weights/best.pt\")\n",
    "\n",
    "# Make predictions on a test image\n",
    "results = model.predict(\"test_tiger.jpg\", save=True)\n",
    "\n",
    "# Display the predicted image\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb346607-4c4d-4e58-a646-8395aba9c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the predict folder\n",
    "predict_dir = \"runs/detect/predict4\"\n",
    "\n",
    "# List all files in the predict folder\n",
    "predicted_files = [f for f in os.listdir(predict_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# Plot each predicted image\n",
    "for file in predicted_files:\n",
    "    pred_img_path = os.path.join(predict_dir, file)\n",
    "    try:\n",
    "        img = Image.open(pred_img_path)\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Predicted: {file}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6366c-a661-4802-b639-09abef0b8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the train folder\n",
    "train5_dir = \"runs/detect/YOLO_5su_fin//\"\n",
    "\n",
    "# List of metrics to plot\n",
    "metrics = [\"F1_curve.png\", \"PR_curve.png\", \"R_curve.png\"]\n",
    "\n",
    "# Plot each metric\n",
    "for metric in metrics:\n",
    "    metric_path = os.path.join(train5_dir, metric)\n",
    "    if os.path.exists(metric_path):\n",
    "        img = Image.open(metric_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Train5: {metric}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: {metric} not found in {train5_dir}.\")\n",
    "\n",
    "\n",
    "# Path to the val folder\n",
    "val_dir = \"runs/detect/train72\"\n",
    "\n",
    "# List of metrics to plot\n",
    "metrics = [\"F1_curve.png\", \"PR_curve.png\", \"R_curve.png\"]\n",
    "\n",
    "# Plot each metric\n",
    "for metric in metrics:\n",
    "    metric_path = os.path.join(val_dir, metric)\n",
    "    if os.path.exists(metric_path):\n",
    "        img = Image.open(metric_path)\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Validation: {metric}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Warning: {metric} not found in {val_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdcc05-8a7a-4345-9839-bc84e835ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Directory containing images\n",
    "images_dir = \"/home/jovyan/__ANIMALS/images\"\n",
    "\n",
    "# Select 3 random images\n",
    "image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "random_images = random.sample(image_files, min(50, len(image_files)))\n",
    "\n",
    "# Print selected images\n",
    "print(\"Selected images for prediction:\")\n",
    "for img in random_images:\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb1892-5c40-4695-9a92-83a5dc60fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(\"runs/detect/YOLO_5su_fin/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4fab9-d78b-439a-9218-762fecf30998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and save results\n",
    "for img in random_images:\n",
    "    img_path = os.path.join(images_dir, img)\n",
    "    results = model.predict(img_path, save=True, save_dir=\"runs/detect/predict\")\n",
    "    print(f\"Prediction saved for {img} in runs/detect/predict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4136577-4013-42fb-9608-3c060d96ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the predict folder\n",
    "predict_dir = \"runs/detect/predict6\"\n",
    "\n",
    "# List all files in the predict folder\n",
    "predicted_files = [f for f in os.listdir(predict_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# Plot each predicted image\n",
    "for file in predicted_files:\n",
    "    pred_img_path = os.path.join(predict_dir, file)\n",
    "    try:\n",
    "        img = Image.open(pred_img_path)\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Predicted: {file}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3f0b6-908f-4cb6-90dd-d1d1c527a60a",
   "metadata": {},
   "source": [
    "#### YOLO 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4b656-d3db-4966-a84a-6a028a4a73fc",
   "metadata": {},
   "source": [
    "## Set Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee5910-4782-49a3-a3ce-7d475f895b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model_name = \"yolov8m.pt\"\n",
    "if not os.path.exists(model_name):\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "else:\n",
    "    print(f\"Loading {model_name}...\")\n",
    "\n",
    "model = YOLO(model_name)\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data=\"/home/jovyan/__ANIMALS/datasets/Y9_AUTO_FIN/animals.yaml\",\n",
    "    epochs=100,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    device=\"cpu\",  # Change to \"0\" if GPU is available\n",
    "    patience=10,\n",
    "    save_period=5,\n",
    "    pretrained=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr0=0.001,\n",
    "    cos_lr=True,\n",
    "    augment=True,\n",
    "    mosaic=1.0,      # Combine four images into one\n",
    "    # hsv_h=0.015,     # Hue adjustment\n",
    "    # hsv_s=0.7,       # Saturation adjustment\n",
    "    # hsv_v=0.4,       # Value (brightness) adjustment\n",
    "    # flipud=0.5,      # 50% chance of flipping upside down\n",
    "    # fliplr=0.5,      # 50% chance of flipping left-right (corrected from flip_lr)\n",
    "    # degrees=10.0,    # Random rotation up to ±10 degrees (corrected from rotate)\n",
    "    # translate=0.1,   # Translate by 10% of image size\n",
    "    # scale=0.5,       # Scale by up to 50%\n",
    "    # cls=0.5,         # Weight for classification loss (to help with imbalance)\n",
    "    # box=7.5,         # Weight for bounding box loss\n",
    "    # dfl=1.5          # Weight for distribution focal loss\n",
    ")\n",
    "\n",
    "print(\"Training completed. Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f6a32-26a2-48fb-9775-ac4c70d09979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6325f-b9f5-4409-853b-b27010c0b65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a954c-e70b-408f-a599-9c57ca8e7abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e059d-cf51-4d12-ad08-ddb1c51c51a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db4aa6-318f-40a3-aa58-8110aedc345b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
